{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9bf3d2-5c23-4fbf-add3-897105c96fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Imports ---\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1473f0ad-79f7-49d3-a267-903fd5f811d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backend: ollama | Model: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: Load environment variables ===\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_BACKEND = os.getenv(\"MODEL_BACKEND\", \"ollama\")  # 'ollama' or 'openai'\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama3.2\")\n",
    "OLLAMA_API_BASE = os.getenv(\"OLLAMA_API_BASE\", \"http://localhost:11434\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Backend: {MODEL_BACKEND} | Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a83f4b-12e5-490b-a355-da48d5f1528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loaded 101 projects\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: Load dataset ===\n",
    "with open(\"projects-embedded.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    base_projects = json.load(f)\n",
    "\n",
    "print(f\"üìö Loaded {len(base_projects)} projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ef6acf-4192-4846-809d-5b3acd6d813a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f5003b96c44e73b9f027bb35f3db88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sushm\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932b02b84f2a4503a80992767b051dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d2251cf5f04c7b9f410499c7af8d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9535d284eb904984867872fda78e97c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2421ad7e0d694fb8ac6cd984007364a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703ab055bdb64e54b3bb7289c0c37541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb16c29668e143539ab33494495be2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f9b44b32374e9da64260908600494e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7e9e98bcf94b998dd67791e67d3a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a515ea26783410287dc56d1c89dc9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d201dc31661a475eaec609783afa0b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === STEP 4: Load embedding model ===\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a24ed5-6c19-4b5f-ba93-1b16f989c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: Unified LLM Call Function ===\n",
    "def call_model(prompt: str) -> str:\n",
    "    \"\"\"Call either Ollama or OpenAI depending on backend selection.\"\"\"\n",
    "    try:\n",
    "        if MODEL_BACKEND == \"ollama\":\n",
    "            response = requests.post(\n",
    "                f\"{OLLAMA_API_BASE}/api/generate\",\n",
    "                json={\"model\": MODEL_NAME, \"prompt\": prompt},\n",
    "                timeout=120\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                raw = response.text.strip()\n",
    "                return raw.split(\":\", 1)[-1].strip() if \":\" in raw else raw\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Ollama Error {response.status_code}: {response.text[:100]}\")\n",
    "                return None\n",
    "\n",
    "        elif MODEL_BACKEND == \"openai\":\n",
    "            import openai\n",
    "            openai.api_key = OPENAI_API_KEY\n",
    "            resp = openai.ChatCompletion.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            return resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"‚ùå Invalid MODEL_BACKEND in .env\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"üö´ Model call failed:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8224128-e836-407e-9dd2-e6badf7293aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Expanding: Instagram Reach Analysis\n",
      "‚ú® Expanding: Scraping laptop data from Amazon\n",
      "‚ú® Expanding: Video Game Sales Prediction\n",
      "‚ú® Expanding: Heart disease detection\n",
      "‚ú® Expanding: Food order prediction\n",
      "‚ú® Expanding: Contact tracing system\n",
      "‚ú® Expanding: Sarcasm detection\n",
      "‚ú® Expanding: Medical insurance price prediction\n",
      "‚ú® Expanding: Credit card clustering\n",
      "‚ú® Expanding: MNIST Data\n",
      "‚ú® Expanding: Real time sentiment analysis\n",
      "‚ú® Expanding: News recommendation system\n",
      "‚ú® Expanding: Calories Burnt Prediction\n",
      "‚ú® Expanding: Online Payment Fraud Detection\n",
      "‚ú® Expanding: Rainfall Prediction system\n",
      "‚ú® Expanding: Health and Fitness Tracking with Gamification\n",
      "‚ú® Expanding: Human-Robot Interaction Interface\n",
      "‚ú® Expanding: Fake News Detection System\n",
      "‚ú® Expanding: E-commerce platform\n",
      "‚ú® Expanding: Smart Traffic Light System\n",
      "‚ú® Expanding: Library Management System\n",
      "‚ú® Expanding: Building AI Chatbots\n",
      "‚ú® Expanding: Image Recognition\n",
      "‚ú® Expanding: Sign Language Recognition System\n",
      "‚ú® Expanding: Recommendation System\n",
      "‚ú® Expanding: Sentiment Analysis Software\n",
      "‚ú® Expanding: Cybersecurity Framework\n",
      "‚ú® Expanding: Augmented reality application\n",
      "‚ú® Expanding: Virtual Private Network\n",
      "‚ú® Expanding: Intrusion Detection System (IDS)\n",
      "‚ú® Expanding: Medical Image Analysis\n",
      "‚ú® Expanding: Online Learning Platform\n",
      "‚ú® Expanding: SMS Spam Filtering\n",
      "‚ú® Expanding: Vulnerability Scanner\n",
      "‚ú® Expanding: Weather Forecasting Application\n",
      "‚ú® Expanding: Smart Farming and Agriculture\n",
      "‚ú® Expanding: A Prediction Tool for Cardiac Diseases\n",
      "‚ú® Expanding: Election Analysis\n",
      "‚ú® Expanding: Beautiful Soup for Web Scraping\n",
      "‚ú® Expanding: Hotel Booking Site\n",
      "‚ú® Expanding: Academic Performance Evaluator\n",
      "‚ú® Expanding: Social Media App\n",
      "‚ú® Expanding: Food Delivery App\n",
      "‚ú® Expanding: Predictive Analysis Tool\n",
      "‚ú® Expanding: Gold Price Prediction System\n",
      "‚ú® Expanding: The applications of ML in disease and pest detection in agriculture\n",
      "‚ú® Expanding: To analyse the implications of deploying autonomous drones\n",
      "‚ú® Expanding: Develop a VR application for a specific type of therapy\n",
      "‚ú® Expanding: A mobile or web application to track a user's carbon footprint\n",
      "‚ú® Expanding: Analysing the vulnerability of cryptographic algorithms to quantum attacks\n",
      "‚ú® Expanding: A Phishing Detection System\n",
      "‚ú® Expanding: Predicting Customer Churn\n",
      "‚ú® Expanding: Self-driving systems simulations\n",
      "‚ú® Expanding: Handwritten Digit Recognition System\n",
      "‚ú® Expanding: Fraud Detection System for Credit Card:\n",
      "‚ú® Expanding: Customer Segmentation Using Clustering:\n",
      "‚ú® Expanding: Blog Management Systems With User Application:\n",
      "‚ú® Expanding: Fitness Tracker App with Step Counter:\n",
      "‚ú® Expanding: Handwritten Digit Recognition Using Neural Network:\n",
      "‚ú® Expanding: Start small with a simple multi-layer perceptron, and then progress to proper CNNs for better accuracy. Play with hyperparameters or different types of data for performance improvement. Great for deep learning and/or imaging, this is an entry-level foray into the realm beyond optical character recognition (OCR).\n",
      "‚ú® Expanding: Fingerprint Reader:\n",
      "‚ú® Expanding: House Price Prediction:\n",
      "‚ú® Expanding: Language Learning App with Gamification:\n",
      "‚ú® Expanding: Biometric Attendance System:\n",
      "‚ú® Expanding: Chatbot Song Recommendation:\n",
      "‚ú® Expanding: Personal Portfolio Website\n",
      "‚ú® Expanding: Homomorphic Encryption Data Analytics Platform\n",
      "‚ú® Expanding: Encrypt your data with a homomorphic encryption library.\n",
      "‚ú® Expanding: Upload that encrypted data to a cloud server.\n",
      "‚ú® Expanding: Perform some simple analytics on that encrypted data (sum or average) from the cloud server.\n",
      "‚ú® Expanding: Decrypt the final answer on your own computer to see the real result.\n",
      "‚ú® Expanding: Secure Multi-party Computation (MPC) for Shared Learning\n",
      "‚ú® Expanding: Post-Quantum VPN Using Lattice Cryptography\n",
      "‚ú® Expanding: Memory-Safe Microkernel with Formal Verification\n",
      "‚ú® Expanding: Hardware Root-of-Trust Using Physical Unclonable Functions (PUFs)\n",
      "‚ú® Expanding: One-Way Diode for Critical Infrastructure\n",
      "‚ú® Expanding: Predict Energy Consumption\n",
      "‚ú® Expanding: Predict Insurance Charges\n",
      "‚ú® Expanding: Predic Credit Card Approvals\n",
      "‚ú® Expanding: Wine Quality Prediction\n",
      "‚ú® Expanding: This project is important for industries involved in wine production and quality control, as it enables them to consistently monitor and predict wine quality, ensuring product excellence.\n",
      "‚ú® Expanding: Store Sales\n",
      "‚ú® Expanding: Reveal Categories Found in Data\n",
      "‚ú® Expanding: Word Frequency in Moby Dick\n",
      "‚ú® Expanding: Facial Recognition with Supervised Learning\n",
      "‚ú® Expanding: Breast Cancer Detection\n",
      "‚ú® Expanding: Speech Emotion Recognition with librosa\n",
      "‚ú® Expanding: Build Rick Sanchez Bot Using Transformers\n",
      "‚ú® Expanding: Building an E-Commerce Clothing Classifier Model with Keras\n",
      "‚ú® Expanding: Detect Traffic Signs with Deep Learning\n",
      "‚ú® Expanding: Stock Market Analysis And Forecasting Using Deep Learning\n",
      "‚ú® Expanding: Reinforcement Learning for Connect X\n",
      "‚ú® Expanding: Multi-Lingual ASR With Transformers\n",
      "‚ú® Expanding: One Shot Face Stylization\n",
      "‚ú® Expanding: H&M Personalized Fashion Recommendations\n",
      "‚ú® Expanding: Reinforcement Learning Agent for Atari 2600\n",
      "‚ú® Expanding: MLOps End-To-End Machine Learning\n",
      "‚ú® Expanding: BERT Text Classifier on Tensor Processing Unit\n",
      "‚ú® Expanding: Image Classification Using Julia\n",
      "‚ú® Expanding: Image Caption Generator\n",
      "‚ú® Expanding: Generate Music using Neural Networks\n",
      "\n",
      "‚úÖ Total dataset size: 404\n",
      "üíæ Saved as: projects-embedding-augmented.json\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: Paraphrasing Function ===\n",
    "def paraphrase_text(text, n=3):\n",
    "    \"\"\"Generate n paraphrased project descriptions.\"\"\"\n",
    "    paras = []\n",
    "    for i in range(n):\n",
    "        prompt = f\"\"\"\n",
    "You are an expert technical writer. Paraphrase the following project description\n",
    "to make it sound unique and professionally reworded while keeping the same meaning.\n",
    "Avoid lists or markdown. Output only the rephrased text.\n",
    "\n",
    "Original:\n",
    "{text}\n",
    "\n",
    "---\n",
    "Paraphrased:\n",
    "\"\"\"\n",
    "        out = call_model(prompt)\n",
    "        if out:\n",
    "            paras.append(out.strip())\n",
    "        time.sleep(1)\n",
    "    return paras\n",
    "\n",
    "# === STEP 7: Expand Dataset ===\n",
    "synthetic_projects = []\n",
    "\n",
    "for idx, proj in enumerate(base_projects, start=1):\n",
    "    base_entry = proj.copy()\n",
    "    synthetic_projects.append(base_entry)\n",
    "\n",
    "    print(f\"‚ú® Expanding: {proj['title']}\")\n",
    "    variations = paraphrase_text(proj[\"description\"], n=3)\n",
    "\n",
    "    for i, v in enumerate(variations, start=1):\n",
    "        new_proj = proj.copy()\n",
    "        new_proj[\"project_id\"] = f\"{proj['project_id']}_syn{i}\"\n",
    "        new_proj[\"description\"] = v\n",
    "\n",
    "        text_for_embed = f\"{new_proj['title']} {v} {new_proj['tech_stack']} {new_proj['objective']}\"\n",
    "        new_proj[\"embedding\"] = embedder.encode(text_for_embed).tolist()\n",
    "\n",
    "        synthetic_projects.append(new_proj)\n",
    "\n",
    "print(f\"\\n‚úÖ Total dataset size: {len(synthetic_projects)}\")\n",
    "\n",
    "# === STEP 8: Save Final Dataset ===\n",
    "with open(\"projects-embedding-augmented.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(synthetic_projects, f, indent=2)\n",
    "\n",
    "print(\"üíæ Saved as: projects-embedding-augmented.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd8bd7a-a6a5-488a-9ce2-a43571e7921a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored 404 projects in ChromaDB (384-dim embeddings).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import json\n",
    "\n",
    "# --- Load model (384 dimensions) ---\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --- Load your dataset ---\n",
    "with open(\"projects-embedding-augmented.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    projects = json.load(f)\n",
    "\n",
    "# --- Initialize ChromaDB ---\n",
    "client = chromadb.PersistentClient(path=\"./chroma_store\")\n",
    "collection = client.get_or_create_collection(name=\"DBMS-25\")\n",
    "\n",
    "# --- Prepare and store ---\n",
    "ids, texts, metadatas, embeddings = [], [], [], []\n",
    "\n",
    "for i, proj in enumerate(projects):\n",
    "    text = f\"{proj['title']} - {proj['description']} - {proj.get('objective', '')}\"\n",
    "    emb = embedder.encode(text).tolist()\n",
    "\n",
    "    ids.append(str(i))\n",
    "    texts.append(text)\n",
    "    embeddings.append(emb)\n",
    "    metadatas.append({\n",
    "        \"title\": proj[\"title\"],\n",
    "        \"domain\": proj[\"domain\"],\n",
    "        \"tech_stack\": proj[\"tech_stack\"],\n",
    "        \"source\": proj[\"source\"]\n",
    "    })\n",
    "\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=texts,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Stored {len(ids)} projects in ChromaDB (384-dim embeddings).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a1ee9c-7cb1-4945-a90f-42ba204e44a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 3 Matches for Query:\n",
      "\n",
      "[1] Fake News Detection System\n",
      "Similarity Score: 1.2879\n",
      "Domain: Artificial Intelligence / Data Science\n",
      "Tech Stack: Python, TensorFlow, PyTorch, NLTK, spaCy, Keras, Pandas, NumPy\n",
      "Source: ISE-dept\n",
      "\n",
      "[2] Fake News Detection System\n",
      "Similarity Score: 1.2883\n",
      "Domain: Artificial Intelligence / Data Science\n",
      "Tech Stack: Python, TensorFlow, PyTorch, NLTK, spaCy, Keras, Pandas, NumPy\n",
      "Source: ISE-dept\n",
      "\n",
      "[3] Fake News Detection System\n",
      "Similarity Score: 1.3045\n",
      "Domain: Artificial Intelligence / Data Science\n",
      "Tech Stack: Python, TensorFlow, PyTorch, NLTK, spaCy, Keras, Pandas, NumPy\n",
      "Source: ISE-dept\n"
     ]
    }
   ],
   "source": [
    "# --- Load same embedding model ---\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --- Connect to ChromaDB ---\n",
    "client = chromadb.PersistentClient(path=\"./chroma_store\")\n",
    "collection = client.get_collection(\"DBMS-25\")\n",
    "\n",
    "# --- Query ---\n",
    "query = \"Disaster-News-Auth-Validate \"\n",
    "query_embedding = embedder.encode(query).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nüîç Top 3 Matches for Query:\")\n",
    "for idx, meta in enumerate(results[\"metadatas\"][0]):\n",
    "    print(f\"\\n[{idx+1}] {meta['title']}\")\n",
    "    print(f\"Similarity Score: {results['distances'][0][idx]:.4f}\")\n",
    "    print(f\"Domain: {meta['domain']}\")\n",
    "    print(f\"Tech Stack: {meta['tech_stack']}\")\n",
    "    print(f\"Source: {meta['source']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ff6ce-5751-4c98-b657-c35eb5d10751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
