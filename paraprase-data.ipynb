{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9bf3d2-5c23-4fbf-add3-897105c96fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Imports ---\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1473f0ad-79f7-49d3-a267-903fd5f811d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backend: ollama | Model: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: Load environment variables ===\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_BACKEND = os.getenv(\"MODEL_BACKEND\", \"ollama\")  # 'ollama' or 'openai'\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama3.2\")\n",
    "OLLAMA_API_BASE = os.getenv(\"OLLAMA_API_BASE\", \"http://localhost:11434\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Backend: {MODEL_BACKEND} | Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a83f4b-12e5-490b-a355-da48d5f1528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loaded 101 projects\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: Load dataset ===\n",
    "with open(\"projects-embedded.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    base_projects = json.load(f)\n",
    "\n",
    "print(f\"üìö Loaded {len(base_projects)} projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ef6acf-4192-4846-809d-5b3acd6d813a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f5003b96c44e73b9f027bb35f3db88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sushm\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932b02b84f2a4503a80992767b051dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d2251cf5f04c7b9f410499c7af8d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9535d284eb904984867872fda78e97c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2421ad7e0d694fb8ac6cd984007364a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703ab055bdb64e54b3bb7289c0c37541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb16c29668e143539ab33494495be2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f9b44b32374e9da64260908600494e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7e9e98bcf94b998dd67791e67d3a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a515ea26783410287dc56d1c89dc9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d201dc31661a475eaec609783afa0b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === STEP 4: Load embedding model ===\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a24ed5-6c19-4b5f-ba93-1b16f989c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: Unified LLM Call Function ===\n",
    "def call_model(prompt: str) -> str:\n",
    "    \"\"\"Call either Ollama or OpenAI depending on backend selection.\"\"\"\n",
    "    try:\n",
    "        if MODEL_BACKEND == \"ollama\":\n",
    "            response = requests.post(\n",
    "                f\"{OLLAMA_API_BASE}/api/generate\",\n",
    "                json={\"model\": MODEL_NAME, \"prompt\": prompt},\n",
    "                timeout=120\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                raw = response.text.strip()\n",
    "                return raw.split(\":\", 1)[-1].strip() if \":\" in raw else raw\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Ollama Error {response.status_code}: {response.text[:100]}\")\n",
    "                return None\n",
    "\n",
    "        elif MODEL_BACKEND == \"openai\":\n",
    "            import openai\n",
    "            openai.api_key = OPENAI_API_KEY\n",
    "            resp = openai.ChatCompletion.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            return resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"‚ùå Invalid MODEL_BACKEND in .env\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"üö´ Model call failed:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8224128-e836-407e-9dd2-e6badf7293aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Expanding: Instagram Reach Analysis\n",
      "‚ú® Expanding: Scraping laptop data from Amazon\n",
      "‚ú® Expanding: Video Game Sales Prediction\n",
      "‚ú® Expanding: Heart disease detection\n",
      "‚ú® Expanding: Food order prediction\n",
      "‚ú® Expanding: Contact tracing system\n",
      "‚ú® Expanding: Sarcasm detection\n",
      "‚ú® Expanding: Medical insurance price prediction\n",
      "‚ú® Expanding: Credit card clustering\n",
      "‚ú® Expanding: MNIST Data\n",
      "‚ú® Expanding: Real time sentiment analysis\n",
      "‚ú® Expanding: News recommendation system\n",
      "‚ú® Expanding: Calories Burnt Prediction\n",
      "‚ú® Expanding: Online Payment Fraud Detection\n",
      "‚ú® Expanding: Rainfall Prediction system\n",
      "‚ú® Expanding: Health and Fitness Tracking with Gamification\n",
      "‚ú® Expanding: Human-Robot Interaction Interface\n",
      "‚ú® Expanding: Fake News Detection System\n",
      "‚ú® Expanding: E-commerce platform\n",
      "‚ú® Expanding: Smart Traffic Light System\n",
      "‚ú® Expanding: Library Management System\n",
      "‚ú® Expanding: Building AI Chatbots\n",
      "‚ú® Expanding: Image Recognition\n",
      "‚ú® Expanding: Sign Language Recognition System\n",
      "‚ú® Expanding: Recommendation System\n",
      "‚ú® Expanding: Sentiment Analysis Software\n",
      "‚ú® Expanding: Cybersecurity Framework\n",
      "‚ú® Expanding: Augmented reality application\n",
      "‚ú® Expanding: Virtual Private Network\n",
      "‚ú® Expanding: Intrusion Detection System (IDS)\n",
      "‚ú® Expanding: Medical Image Analysis\n",
      "‚ú® Expanding: Online Learning Platform\n",
      "‚ú® Expanding: SMS Spam Filtering\n",
      "‚ú® Expanding: Vulnerability Scanner\n",
      "‚ú® Expanding: Weather Forecasting Application\n",
      "‚ú® Expanding: Smart Farming and Agriculture\n",
      "‚ú® Expanding: A Prediction Tool for Cardiac Diseases\n",
      "‚ú® Expanding: Election Analysis\n",
      "‚ú® Expanding: Beautiful Soup for Web Scraping\n",
      "‚ú® Expanding: Hotel Booking Site\n",
      "‚ú® Expanding: Academic Performance Evaluator\n",
      "‚ú® Expanding: Social Media App\n",
      "‚ú® Expanding: Food Delivery App\n",
      "‚ú® Expanding: Predictive Analysis Tool\n",
      "‚ú® Expanding: Gold Price Prediction System\n",
      "‚ú® Expanding: The applications of ML in disease and pest detection in agriculture\n",
      "‚ú® Expanding: To analyse the implications of deploying autonomous drones\n",
      "‚ú® Expanding: Develop a VR application for a specific type of therapy\n",
      "‚ú® Expanding: A mobile or web application to track a user's carbon footprint\n",
      "‚ú® Expanding: Analysing the vulnerability of cryptographic algorithms to quantum attacks\n",
      "‚ú® Expanding: A Phishing Detection System\n",
      "‚ú® Expanding: Predicting Customer Churn\n",
      "‚ú® Expanding: Self-driving systems simulations\n",
      "‚ú® Expanding: Handwritten Digit Recognition System\n",
      "‚ú® Expanding: Fraud Detection System for Credit Card:\n",
      "‚ú® Expanding: Customer Segmentation Using Clustering:\n",
      "‚ú® Expanding: Blog Management Systems With User Application:\n",
      "‚ú® Expanding: Fitness Tracker App with Step Counter:\n",
      "‚ú® Expanding: Handwritten Digit Recognition Using Neural Network:\n",
      "‚ú® Expanding: Start small with a simple multi-layer perceptron, and then progress to proper CNNs for better accuracy. Play with hyperparameters or different types of data for performance improvement. Great for deep learning and/or imaging, this is an entry-level foray into the realm beyond optical character recognition (OCR).\n",
      "‚ú® Expanding: Fingerprint Reader:\n",
      "‚ú® Expanding: House Price Prediction:\n",
      "‚ú® Expanding: Language Learning App with Gamification:\n",
      "‚ú® Expanding: Biometric Attendance System:\n",
      "‚ú® Expanding: Chatbot Song Recommendation:\n",
      "‚ú® Expanding: Personal Portfolio Website\n",
      "‚ú® Expanding: Homomorphic Encryption Data Analytics Platform\n",
      "‚ú® Expanding: Encrypt your data with a homomorphic encryption library.\n",
      "‚ú® Expanding: Upload that encrypted data to a cloud server.\n",
      "‚ú® Expanding: Perform some simple analytics on that encrypted data (sum or average) from the cloud server.\n",
      "‚ú® Expanding: Decrypt the final answer on your own computer to see the real result.\n",
      "‚ú® Expanding: Secure Multi-party Computation (MPC) for Shared Learning\n",
      "‚ú® Expanding: Post-Quantum VPN Using Lattice Cryptography\n",
      "‚ú® Expanding: Memory-Safe Microkernel with Formal Verification\n",
      "‚ú® Expanding: Hardware Root-of-Trust Using Physical Unclonable Functions (PUFs)\n",
      "‚ú® Expanding: One-Way Diode for Critical Infrastructure\n",
      "‚ú® Expanding: Predict Energy Consumption\n",
      "‚ú® Expanding: Predict Insurance Charges\n",
      "‚ú® Expanding: Predic Credit Card Approvals\n",
      "‚ú® Expanding: Wine Quality Prediction\n",
      "‚ú® Expanding: This project is important for industries involved in wine production and quality control, as it enables them to consistently monitor and predict wine quality, ensuring product excellence.\n",
      "‚ú® Expanding: Store Sales\n",
      "‚ú® Expanding: Reveal Categories Found in Data\n",
      "‚ú® Expanding: Word Frequency in Moby Dick\n",
      "‚ú® Expanding: Facial Recognition with Supervised Learning\n",
      "‚ú® Expanding: Breast Cancer Detection\n",
      "‚ú® Expanding: Speech Emotion Recognition with librosa\n",
      "‚ú® Expanding: Build Rick Sanchez Bot Using Transformers\n",
      "‚ú® Expanding: Building an E-Commerce Clothing Classifier Model with Keras\n",
      "‚ú® Expanding: Detect Traffic Signs with Deep Learning\n",
      "‚ú® Expanding: Stock Market Analysis And Forecasting Using Deep Learning\n",
      "‚ú® Expanding: Reinforcement Learning for Connect X\n",
      "‚ú® Expanding: Multi-Lingual ASR With Transformers\n",
      "‚ú® Expanding: One Shot Face Stylization\n",
      "‚ú® Expanding: H&M Personalized Fashion Recommendations\n",
      "‚ú® Expanding: Reinforcement Learning Agent for Atari 2600\n",
      "‚ú® Expanding: MLOps End-To-End Machine Learning\n",
      "‚ú® Expanding: BERT Text Classifier on Tensor Processing Unit\n",
      "‚ú® Expanding: Image Classification Using Julia\n",
      "‚ú® Expanding: Image Caption Generator\n",
      "‚ú® Expanding: Generate Music using Neural Networks\n",
      "\n",
      "‚úÖ Total dataset size: 404\n",
      "üíæ Saved as: projects-embedding-augmented.json\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: Paraphrasing Function ===\n",
    "def paraphrase_text(text, n=3):\n",
    "    \"\"\"Generate n paraphrased project descriptions.\"\"\"\n",
    "    paras = []\n",
    "    for i in range(n):\n",
    "        prompt = f\"\"\"\n",
    "You are an expert technical writer. Paraphrase the following project description\n",
    "to make it sound unique and professionally reworded while keeping the same meaning.\n",
    "Avoid lists or markdown. Output only the rephrased text.\n",
    "\n",
    "Original:\n",
    "{text}\n",
    "\n",
    "---\n",
    "Paraphrased:\n",
    "\"\"\"\n",
    "        out = call_model(prompt)\n",
    "        if out:\n",
    "            paras.append(out.strip())\n",
    "        time.sleep(1)\n",
    "    return paras\n",
    "\n",
    "# === STEP 7: Expand Dataset ===\n",
    "synthetic_projects = []\n",
    "\n",
    "for idx, proj in enumerate(base_projects, start=1):\n",
    "    base_entry = proj.copy()\n",
    "    synthetic_projects.append(base_entry)\n",
    "\n",
    "    print(f\"‚ú® Expanding: {proj['title']}\")\n",
    "    variations = paraphrase_text(proj[\"description\"], n=3)\n",
    "\n",
    "    for i, v in enumerate(variations, start=1):\n",
    "        new_proj = proj.copy()\n",
    "        new_proj[\"project_id\"] = f\"{proj['project_id']}_syn{i}\"\n",
    "        new_proj[\"description\"] = v\n",
    "\n",
    "        text_for_embed = f\"{new_proj['title']} {v} {new_proj['tech_stack']} {new_proj['objective']}\"\n",
    "        new_proj[\"embedding\"] = embedder.encode(text_for_embed).tolist()\n",
    "\n",
    "        synthetic_projects.append(new_proj)\n",
    "\n",
    "print(f\"\\n‚úÖ Total dataset size: {len(synthetic_projects)}\")\n",
    "\n",
    "# === STEP 8: Save Final Dataset ===\n",
    "with open(\"projects-embedding-augmented.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(synthetic_projects, f, indent=2)\n",
    "\n",
    "print(\"üíæ Saved as: projects-embedding-augmented.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56792c34-07fa-4785-b296-b7912d84b5d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 404 projects\n",
      "‚ö†Ô∏è Inconsistent embedding dimensions found: {1024, 768}\n",
      "üîß Normalizing all to 768D...\n",
      "‚úÖ All embeddings normalized to 768 dimensions\n",
      "üì¶ Added 25 / 404\n",
      "üì¶ Added 50 / 404\n",
      "üì¶ Added 75 / 404\n",
      "üì¶ Added 100 / 404\n",
      "üì¶ Added 125 / 404\n",
      "üì¶ Added 150 / 404\n",
      "üì¶ Added 175 / 404\n",
      "üì¶ Added 200 / 404\n",
      "üì¶ Added 225 / 404\n",
      "üì¶ Added 250 / 404\n",
      "üì¶ Added 275 / 404\n",
      "üì¶ Added 300 / 404\n",
      "üì¶ Added 325 / 404\n",
      "üì¶ Added 350 / 404\n",
      "üì¶ Added 375 / 404\n",
      "üì¶ Added 400 / 404\n",
      "üì¶ Added 404 / 404\n",
      "üíæ All data stored successfully in ChromaDB at ./chroma_store/ (auto-persisted)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushm\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 79.3M/79.3M [00:14<00:00, 5.86MiB/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Collection expecting embedding with dimension of 768, got 384",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# --- Optional: Quick retrieval test ---\u001b[39;00m\n\u001b[32m     77\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33manalyze instagram reach using machine learning\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m results = \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m     81\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîç Top Matches for Query:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, meta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results[\u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\models\\Collection.py:225\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    190\u001b[39m \n\u001b[32m    191\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    210\u001b[39m \n\u001b[32m    211\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m query_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_query_request(\n\u001b[32m    214\u001b[39m     query_embeddings=query_embeddings,\n\u001b[32m    215\u001b[39m     query_texts=query_texts,\n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m     include=include,\n\u001b[32m    223\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere_document\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    238\u001b[39m     response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    239\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\rust.py:519\u001b[39m, in \u001b[36mRustBindingsAPI._query\u001b[39m\u001b[34m(self, collection_id, query_embeddings, ids, n_results, where, where_document, include, tenant, database)\u001b[39m\n\u001b[32m    503\u001b[39m filtered_ids_amount = \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    505\u001b[39m     CollectionQueryEvent(\n\u001b[32m    506\u001b[39m         collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    516\u001b[39m     )\n\u001b[32m    517\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m rust_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[32m    532\u001b[39m     ids=rust_response.ids,\n\u001b[32m    533\u001b[39m     embeddings=rust_response.embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m     distances=rust_response.distances,\n\u001b[32m    540\u001b[39m )\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Collection expecting embedding with dimension of 768, got 384"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import json\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# --- Initialize Chroma client ---\n",
    "client = chromadb.Client(Settings(\n",
    "    persist_directory=\"./chroma_store\",  # local persistence\n",
    "))\n",
    "\n",
    "# --- Create or get collection ---\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"DBMS-RAG\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # cosine similarity works well for embeddings\n",
    ")\n",
    "\n",
    "# --- Load your dataset ---\n",
    "with open(\"projects-embedding-augmented.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    projects = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(projects)} projects\")\n",
    "\n",
    "# --- Prepare data for insertion ---\n",
    "ids = []\n",
    "texts = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "for proj in projects:\n",
    "    emb = proj.get(\"synthetic_embedding\") or proj.get(\"embedding\")\n",
    "    if not emb:\n",
    "        print(f\"‚ö†Ô∏è Skipping {proj['project_id']} (no embedding found)\")\n",
    "        continue\n",
    "\n",
    "    # Validate numeric list embeddings only\n",
    "    if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb):\n",
    "        print(f\"‚ö†Ô∏è Skipping {proj['project_id']} (invalid embedding format)\")\n",
    "        continue\n",
    "\n",
    "    ids.append(proj[\"project_id\"])\n",
    "    texts.append(f\"{proj['title']} - {proj['description']} - {proj.get('objective','')}\")\n",
    "    embeddings.append(emb)\n",
    "    metadatas.append({\n",
    "        \"title\": proj[\"title\"],\n",
    "        \"domain\": proj[\"domain\"],\n",
    "        \"year\": proj[\"year\"],\n",
    "        \"tech_stack\": proj[\"tech_stack\"],\n",
    "        \"source\": proj[\"source\"]\n",
    "    })\n",
    "\n",
    "# --- Check dimension consistency ---\n",
    "dims = [len(e) for e in embeddings]\n",
    "if len(set(dims)) > 1:\n",
    "    print(f\"‚ö†Ô∏è Inconsistent embedding dimensions found: {set(dims)}\")\n",
    "    mode_dim = max(set(dims), key=dims.count)\n",
    "    print(f\"üîß Normalizing all to {mode_dim}D...\")\n",
    "    embeddings = [e[:mode_dim] if len(e) > mode_dim else e + [0]*(mode_dim-len(e)) for e in embeddings]\n",
    "\n",
    "print(f\"‚úÖ All embeddings normalized to {len(embeddings[0])} dimensions\")\n",
    "\n",
    "# --- Add data to ChromaDB ---\n",
    "batch_size = 25\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    collection.add(\n",
    "        ids=ids[i:i+batch_size],\n",
    "        documents=texts[i:i+batch_size],\n",
    "        embeddings=embeddings[i:i+batch_size],\n",
    "        metadatas=metadatas[i:i+batch_size]\n",
    "    )\n",
    "    print(f\"üì¶ Added {min(i+batch_size, len(ids))} / {len(ids)}\")\n",
    "\n",
    "# --- Persistence handled automatically ---\n",
    "print(\"üíæ All data stored successfully in ChromaDB at ./chroma_store/ (auto-persisted)\")\n",
    "\n",
    "# --- Optional: Quick retrieval test ---\n",
    "query = \"analyze instagram reach using machine learning\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Top Matches for Query:\")\n",
    "for idx, meta in enumerate(results[\"metadatas\"][0]):\n",
    "    print(f\"{idx+1}. {meta['title']}  |  Domain: {meta['domain']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9963d987-e614-4478-a291-0c6d7c4ecdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb1594e87a840e8bee001abeb2c6814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sushm\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5748a3a46fc4597995f98e37200ced0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7567672f860347bb82bac3def753495d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560926e8b46d4612a65ff97135a10b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f61b385755491f8c031f4314dbdd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93197e9c7e334a16b0eaee20c813e997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557fb3744c894e2392a3301d3269e1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be66b01c6a94bf4995a229582e69504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4cb6926117407799d715dc8d48cce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c20f1ebf7f4fb0b3a14b5ff71e525a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed791eb407834e7cbd088974d4b70830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Collection expecting embedding with dimension of 768, got 384",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m query_vec = embedding_model.encode([query])[\u001b[32m0\u001b[39m].tolist()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# --- run query ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m results = \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_vec\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîç Top Matches for Query:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, meta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results[\u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\models\\Collection.py:225\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    190\u001b[39m \n\u001b[32m    191\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    210\u001b[39m \n\u001b[32m    211\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m query_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_query_request(\n\u001b[32m    214\u001b[39m     query_embeddings=query_embeddings,\n\u001b[32m    215\u001b[39m     query_texts=query_texts,\n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m     include=include,\n\u001b[32m    223\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere_document\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    238\u001b[39m     response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    239\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\rust.py:519\u001b[39m, in \u001b[36mRustBindingsAPI._query\u001b[39m\u001b[34m(self, collection_id, query_embeddings, ids, n_results, where, where_document, include, tenant, database)\u001b[39m\n\u001b[32m    503\u001b[39m filtered_ids_amount = \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    505\u001b[39m     CollectionQueryEvent(\n\u001b[32m    506\u001b[39m         collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    516\u001b[39m     )\n\u001b[32m    517\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m rust_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[32m    532\u001b[39m     ids=rust_response.ids,\n\u001b[32m    533\u001b[39m     embeddings=rust_response.embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m     distances=rust_response.distances,\n\u001b[32m    540\u001b[39m )\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Collection expecting embedding with dimension of 768, got 384"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# --- load the same embedding model you used for dataset ---\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# --- sample query ---\n",
    "query = \"analyze instagram reach using machine learning\"\n",
    "\n",
    "# --- embed query to match stored vectors ---\n",
    "query_vec = embedding_model.encode([query])[0].tolist()\n",
    "\n",
    "# --- run query ---\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_vec],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Top Matches for Query:\")\n",
    "for idx, meta in enumerate(results[\"metadatas\"][0]):\n",
    "    print(f\"{idx+1}. {meta['title']}  |  Domain: {meta['domain']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8bd7a-a6a5-488a-9ce2-a43571e7921a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
